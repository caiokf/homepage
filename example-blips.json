[
  {
    "name": "TypeScript",
    "ring": "proven",
    "quadrant": "tech-stack",
    "isNew": false,
    "status": "no change",
    "description": "<p> <strong><a href=\"https://www.typescriptlang.org/\">TypeScript</a></strong> has been central to my engineering practice since 2019. Having transitioned incrementally from JavaScript, I now consider TypeScript the baseline for all serious application development. Its static typing model consistently surfaces issues earlier in the process and delivers tangible maintainability benefits at scale. The ecosystem's maturity and seamless integration with modern tooling, the long-term productivity gains and reliability have proven valuable across teams and complex domains. </p>"
  },
  {
    "name": "OpenTelemetry",
    "ring": "proven",
    "quadrant": "tech-stack",
    "isNew": false,
    "status": "moved in",
    "description": "<p> <strong><a href=\"https://opentelemetry.io/\">OpenTelemetry</a></strong> is now my go-to standard for capturing metrics and distributed traces, primarily in conjunction with the Datadog agent. Its unified instrumentation model simplifies integration across diverse languages and services, with out-of-the-box compatibility streamlining observability pipelines. OpenTelemetry has proven reliable for building portable, vendor-neutral observability into all new cloud-native architectures. </p>"
  },
  {
    "name": "NX",
    "ring": "proven",
    "quadrant": "tools",
    "isNew": false,
    "status": "no change",
    "description": "<p> <strong><a href=\"https://nx.dev/\">NX</a></strong> is my preferred toolkit for managing large, polyglot monorepos. Its support for multiple languages and frameworks has enabled me to coordinate TypeScript, Go, Python, and frontend codebases within a single repository while maintaining clear boundaries and scalable build pipelines. Pairing NX with pnpm workspaces provides efficient package management and dependency hoisting, significantly reducing install times and resource usage. NX's computation caching and granular task orchestration have become essential for keeping CI/CD pipelines fast and reliable as projects scale. While the learning curve can be steeper for teams unfamiliar with monorepo tooling, the long-term gains in developer velocity and code organization have justified the investment. </p>"
  },
  {
    "name": "Temporal",
    "ring": "proven",
    "quadrant": "tech-stack",
    "isNew": false,
    "status": "no change",
    "description": "<p> <strong><a href=\"https://temporal.io/\">Temporal</a></strong> has become my default choice for orchestrating durable, reliable workflows in distributed systems. Its ability to provide fault-tolerant state management and guarantee workflow execution—across retries, failures, and restarts—significantly simplifies building robust backend services. I rely on Temporal to handle long-running and multi-step business processes that demand resilience beyond what queues or basic job runners can provide. While adopting Temporal introduces new operational considerations, its powerful abstractions and growing multi-language support have made it the foundation of my approach to durable execution. </p>",
    "messageToConvey": "Durable execution is a game changer, debugging and recovering from failures is infinitely easier. I like temporal a lot, it is robust and the management UI is great. Typescript SDK is ok, not as good as golang one. I wish it was a bit more lightweight and faster to setup, and that it didn't require a container to run. Some workloads would be better on a lambda, and for this is am experimenting with Restate"
  },
  {
    "name": "Vue.js",
    "ring": "proven",
    "quadrant": "tech-stack",
    "isNew": false,
    "status": "moved in",
    "description": "<p><strong><a href=\"https://vuejs.org/\">Vue.js</a></strong> has become my go-to framework for frontend development, and I've consistently valued its approachable learning curve combined with its powerful reactivity system and performant rendering engine. The Vue 3 Composition API has transformed how I architect scalable, maintainable applications, providing excellent TypeScript support that integrates seamlessly with my development workflow. I find the component-based single-file component structure particularly effective for accelerating modern UI development and building SPAs. In my experience, Vue's MVVM architecture, two-way data binding, and intuitive template syntax make it exceptionally well suited for both integrating with legacy systems and building new greenfield applications. I appreciate the flexibility of its ecosystem—from state management to routing to server rendering—and the robust documentation and rich third-party support have consistently enabled me to deliver high-quality solutions across diverse project contexts.</p>"
  },
  {
    "name": "Pulumi",
    "ring": "proven",
    "quadrant": "tech-stack",
    "isNew": false,
    "status": "no change",
    "description": "<p> <strong><a href=\"https://www.pulumi.com/\">Pulumi</a></strong> is my default infrastructure-as-code platform for cloud automation. I value its multi-cloud support and ability to define infrastructure using standard programming languages instead of domain-specific configuration (YAML, HCL). Pulumi's approach enables more expressive, testable, and reusable infrastructure code, using established patterns and libraries familiar from application development. Compared to Terraform and AWS CDK, Pulumi is the most seamless for managing resources across providers and for integrating infrastructure definitions directly into CI/CD toolchains and shared codebases. The productivity boost from leveraging IDE features, package management, and language-native abstractions has made Pulumi my preferred choice for both greenfield and complex hybrid-cloud projects. </p>",
    "messageToConvey": "my preferred iac tool. writing in typescript (or same language as the codebase) is a great advantage, i feel. multi cloud support is a must for me, so i prefer it over cdk. sometimes creating non-existing providers is a pain, but it is getting more manageable with AI. Some times is gives me grief when having to rename logical pulumi component names, because pulumi tries to redeploy them, so it is important to think properly about pulumi component tree hierarchy and naming conventions."
  },
  {
    "name": "Google BigQuery",
    "ring": "proven",
    "quadrant": "tech-stack",
    "isNew": false,
    "status": "no change",
    "description": "<p> <strong><a href=\"https://cloud.google.com/bigquery\">Google BigQuery</a></strong> is my primary choice for managed cloud analytics and data warehousing. BigQuery's serverless architecture, automatic scaling, and straightforward SQL interface make it ideal for consuming structured data at scale without worrying about infrastructure provisioning. I've found its separation of storage and compute, native support for streaming ingestion, and tight integration with other GCP services to be practical differentiators for rapid data delivery and exploration. <br><br> Compared to Redshift and Snowflake, BigQuery's zero-ops approach removes much of the operational overhead—there's no need to tune clusters or manually allocate resources. While Redshift tends to require more active monitoring and Snowflake leads on cross-cloud portability and semi-structured data support, I appreciate BigQuery's seamless scaling and predictable cost structure</p>",
    "messageToConvey": "i love bigquery, it is a great tool for data analysis and reporting. it can be expensive, but especially on small teams/startups, it can delay the hiring of many roles, due to the nice ecosystem and ease to use. BQ ML is also great and easy to use for initial ML projects/needs, before a ML team is formed."
  },
  {
    "name": "Datadog",
    "ring": "proven",
    "quadrant": "tools",
    "isNew": false,
    "status": "no change",
    "description": "<p> <strong><a href=\"https://www.datadoghq.com/\">Datadog</a></strong> is my go-to observability and monitoring platform, offering industry-leading coverage across metrics, traces, dashboards, and alerting in a unified SaaS solution. Datadog's breadth of integrations and ease of setup allow me to achieve comprehensive visibility across cloud infrastructure, applications, and services. <br><br> That said, Datadog's cost escalates quickly with growing data volumes. For most organizations I work with, its advantages far outweigh the expense—up until the point where scale drives costs beyond what can be justified. At large scale, some companies opt to invest in custom platforms (built on OSS components like Prometheus, Grafana, OpenSearch, or internal tools) to regain cost control and tailor observability to their specific needs. One feature I do miss, compared to tools like Lumigo, is more granular alerting on discrete failures—such as individual Lambda invocation errors—which can be invaluable for diagnosing issues in event-driven architectures.</p>",
    "messageToConvey": "it is the best, but very expensive and can easily get costs out of hand. especially on small teams/startups, it can speed up monitoring practices. it has good iac support, i miss not having alerts on individual lambda failures like Lumigo does"
  },
  {
    "name": "Neon",
    "ring": "experimental",
    "quadrant": "tech-stack",
    "isNew": false,
    "status": "moved in",
    "description": "<p><strong><a href=\"https://neon.tech/\">Neon</a></strong> has become my preferred serverless Postgres platform for cloud-native development. I value its instant scaling capabilities, database branching, and the separation of storage and compute that simplifies my architecture decisions. In my experience, Neon's branching feature has transformed my CI/CD workflows—I can now spin up isolated database environments for testing and preview deployments in seconds rather than minutes. The developer-friendly API and rapid deployment have eliminated much of the operational overhead I've found burdensome with traditional managed database solutions. I've found Neon to be exceptionally well-suited for data-driven applications where I need both the reliability of Postgres and the agility of modern serverless infrastructure.</p>",
    "messageToConvey": "i love neon, it is a great tool for cloud-native development. it is fast, and has a great scaling capabilities. i jsut wish there was a fully supported pulumi provider."
  },
  {
    "name": "Event Sourcing",
    "ring": "proven",
    "quadrant": "techniques",
    "isNew": false,
    "status": "no change",
    "description": "<p><strong>Event Sourcing</strong> has become my preferred architectural pattern for modeling state changes as atomic events. I've found that storing all changes as append-only events delivers complete auditability, flexibility for building projections, and robust support for event-driven operations. In my experience working across domains like finance and IoT, event sourcing has consistently proven valuable for traceability, resilience, and orchestrating complex workflows. That said, I've learned it requires careful design around event schema evolution and eventual consistency to avoid technical debt.</p>",
    "messageToConvey": "i find it hard to work on any reasonably large system without event sourcing. modeling state changes as first class citizens has some many benefits since development, debugging, disaster recovery all the way to data engineering, which is hard to beat. use for everything but the most basic apps. Usually use DynamoDB as my default event store."
  },
  {
    "name": "Data Mesh",
    "ring": "proven",
    "quadrant": "techniques",
    "isNew": false,
    "status": "moved in",
    "description": "<p><strong><a href=\"https://datamesh.com/\">Data mesh</a></strong> has become my preferred architectural approach for data sharing, management, and analytics at scale, emphasizing domain-oriented ownership and decentralized governance. In my experience, while the concept generates significant interest and teams are eager to adopt mesh-enabled patterns, I've observed that actual implementations remain challenging—most organizations I've worked with only deploy it in isolated pockets due to high integration costs, cultural resistance to decentralization, and the complexity of federated governance. I've found the most common barriers include maintaining interoperability across domains, preventing new data silos, upskilling domain teams to own their data products, and ensuring consistent quality control through federation. What I value most about data mesh is its recognition that technology alone isn't enough—the sociotechnical aspects like domain-driven contracts, self-service infrastructure platforms, and federated computational governance are what I've found truly drive success, yet these are often under-emphasized in practice. Resources like \"Data Mesh: Delivering Data-Driven Value at Scale\" have been invaluable guides in my journey, helping me navigate the patterns, architectural trade-offs, and organizational design decisions required to move beyond traditional centralized data architectures toward a truly robust, decentralized strategy.</p>",
    "messageToConvey": "good for teams to own their own data instead of dedicated data engineering teams. requires goood established patterns to work well across teams."
  },
  {
    "name": "Platform engineering product teams",
    "ring": "experimental",
    "quadrant": "techniques",
    "isNew": false,
    "status": "no change",
    "description": "<p><strong>Platform engineering product teams</strong> have become my preferred organizational pattern for addressing the complexity of modern cloud and DevOps environments. In my experience, treating internal platforms as products—with dedicated teams building developer-centric tools, automation, and API-driven self-service—has consistently delivered better outcomes than traditional centralized operations models. I've found that when platform teams focus on genuine product ownership and iterative improvement based on developer feedback, they meaningfully reduce lead time, boost productivity, and eliminate operational bottlenecks, while stream-aligned delivery teams retain ownership of their deployed workloads. The key distinction I value is treating the platform as a product with real customers (developers), not just infrastructure. Leveraging Team Topologies concepts, I've seen organizations successfully split platform responsibilities among enablement, core, and stream-aligned teams, establishing clear boundaries and collaboration patterns that accelerate delivery rather than create new silos.</p>",
    "messageToConvey": "it is a movement that is gaining momentum. I think it is a good way to organize teams. On a startup environment, this might mean a much more lean approach and early on, team buy-in and feedback in essential, but I still think a lightweight version of this is very helpful."
  },
  {
    "name": "Curated shared instructions for software teams",
    "ring": "experimental",
    "quadrant": "ai",
    "isNew": true,
    "status": "new",
    "description": "<p>In my experience working with AI-assisted software delivery, I've found that moving beyond individual prompting toward <strong>curated shared instructions for software teams</strong> has become essential. This practice has enabled me to apply AI effectively across all delivery tasks — not just coding — by building a library of proven, high-quality instructions. My preferred approach is committing instruction files, such as an <a href=\"/radar/techniques/agents-md\">AGENTS.md</a>, directly to project repositories. support sharing instructions through custom slash commands or workflows. For noncoding tasks, I've set up organization-wide prompt libraries ready to use. I appreciate this systematic approach for continuous improvement: As soon as I refine a prompt, my entire team benefits, ensuring consistent access to our best AI instructions.</p>",
    "messageToConvey": "nowadays with AI evolving so quickly, i think it is important that every team member practice and experiment with different tools that fit their workflow, in a way that AI tools are an extension of themselves. This fans out the learning process and speeds up. That said, having a good cadence of engineering meetup in the company, where proven/efficient patterns are shared and discussed, and therefore included in the project's source control, is essential so that not only everyone can benefit from same practices, but also, that ai usage can scale and the codebase looks homogeneous."
  },
  {
    "name": "Platform'ed Webhooks",
    "ring": "proven",
    "quadrant": "techniques",
    "isNew": true,
    "status": "new",
    "description": "<p><strong>Platform'ed Webhooks</strong> has become my go-to pattern for managing webhook integrations at scale. I've found that building generic webhook handlers and routing them through a centralized event bus dramatically simplifies how I connect internal and third-party systems in event-driven architectures. In my experience, this approach makes onboarding integration partners significantly easier while ensuring I maintain consistent security and observability across all webhook endpoints. I value how it reduces the maintenance burden—rather than managing dozens of specialized webhook handlers, I can leverage a unified platform that provides reliability and agility when connecting SaaS tools and microservices ecosystems.</p>",
    "messageToConvey": "started having third party webhooks consistently come through a platform handler, which streams it to an internal event bus and it is a great way to manage webhook integrations at scale. it pays off immediately. the ease to not only handle different events, but also do things like data warehouse ingestion."
  },
  {
    "name": "Cursor",
    "ring": "proven",
    "quadrant": "ai",
    "isNew": false,
    "status": "moved in",
    "description": "<p><strong><a href=\"https://www.cursor.so/\">Cursor</a></strong> is an AI-driven code editor that augments the developer workflow with smart code suggestions, refactoring, and proactive error detection. Cursor stands out for its seamless integration with popular languages and its ability to improve productivity, especially in large projects. Teams have reported significant improvements in code quality and delivery speed by incorporating Cursor into their engineering toolchains.</p>",
    "messageToConvey": "i found it the best code completion there is, and agent mode is great for the more common daily tasks. Rule loading works very well and very flexible. For more complex tasks, i prefer Claude Code at the moment of writing this."
  },
  {
    "name": "K9s",
    "ring": "proven",
    "quadrant": "tools",
    "isNew": false,
    "status": "moved in",
    "description": "<p> <strong><a href=\"https://k9s.io/\">k9s</a></strong> is my preferred utility for interacting with and debugging Kubernetes clusters. Its terminal-based interface delivers fast, intuitive access to workloads, logs, events, and resource details, streamlining daily cluster management tasks without the overhead of graphical dashboards. k9s excels at surfacing pod-level status and troubleshooting issues in real time, and is particularly valuable during incident response or development cycles where speed and clarity are critical. For engineers familiar with command-line workflows, k9s has become indispensable for efficient Kubernetes operations, supplementing (or replacing) kubectl and web UIs across a wide range of environments. </p>",
    "messageToConvey": "i love k9s and it makes for debugging kubernetes a breeze. much better and easier than kubectl. tui is easy to use."
  },
  {
    "name": "GitHub Actions",
    "ring": "proven",
    "quadrant": "tools",
    "isNew": false,
    "status": "no change",
    "description": "<p><strong><a href=\"https://github.com/features/actions\">GitHub Actions</a></strong> has become my go-to CI/CD platform for virtually every project I work on. I value its deep integration with the GitHub ecosystem—being able to trigger workflows directly from PRs, pushes, and issues eliminates the friction of connecting external tools and keeps everything in one place. In my experience, the YAML-based configuration strikes the right balance between flexibility and readability, while the marketplace of reusable actions has saved me countless hours of reinventing common patterns. I've found features like matrix builds, self-hosted runners, parallel jobs, and advanced caching to be powerful enough for both my small open-source projects and large-scale enterprise deployments. My preferred aspect of GitHub Actions is how it consistently delivers a smooth developer experience while evolving rapidly with new capabilities, making it the foundation of my modern software delivery pipelines.</p>",
    "messageToConvey": "default ci tool, being in a github repo. not much to say"
  },
  {
    "name": "Tuple",
    "ring": "proven",
    "quadrant": "tools",
    "isNew": false,
    "status": "moved in",
    "description": "<p> <strong><a href=\"https://tuple.app/\">Tuple</a></strong> is my preferred solution for high-fidelity remote pairing and collaborative problem-solving. It stands out for its low-latency screen sharing—capable of selecting individual windows or screen regions—and provides seamless shared control, allowing collaborators to easily hand off keyboard and mouse input. Tuple's in-session annotation, chat, and synchronized code editing features go beyond standard video conferencing tools, making real-time review, debugging, and co-writing sessions highly productive. For distributed teams or cross-functional work, Tuple enables fast, natural technical collaboration that closely replicates the experience of in-person pairing. </p>",
    "messageToConvey": "best remote pairing tool i have tried. good tools for remote control, screen annotation, chat, and having a large monitor 57 ultrawide, i especially like/need the share a portion of the screen feature"
  },
  {
    "name": "pnpm",
    "ring": "proven",
    "quadrant": "tools",
    "isNew": false,
    "status": "moved in",
    "description": "<p><strong><a href=\"https://pnpm.js.org/\">pnpm</a></strong> has become my preferred <a href=\"/radar/platforms/node-js\">Node.js</a> package manager, delivering performance improvements I've found to be significant both in speed and disk space efficiency. I value how it hard-links duplicate packages from multiple projects' <code>node_modules</code> folders to a single location on disk and supports incremental, file-level optimizations that further boost performance. In my experience, pnpm offers a much faster feedback loop with minimal compatibility issues, making it my go-to choice for Node.js package management across all projects I work on.</p>",
    "messageToConvey": "pnpm is a great package manager for node.js. it is fast, and has a great feedback loop. it is my go-to choice for node.js package management across all projects i work on."
  },
  {
    "name": "esbuild",
    "ring": "proven",
    "quadrant": "tools",
    "isNew": false,
    "status": "no change",
    "description": "<p><strong><a href=\"https://esbuild.github.io/\">esbuild</a></strong> has become my go-to bundler for JavaScript projects, and I've found its performance to be transformative for developer productivity. In my experience, the speed—often measured in milliseconds—eliminates the build-time friction that I used to tolerate with other bundlers. I value how straightforward it is to set up and integrate into CI/CD pipelines, with minimal configuration required to get started. Since it's written in Go and supports JavaScript, TypeScript, CSS, and JSX out of the box, I've been able to rely on it across diverse project types—from libraries to full web applications. I appreciate that tree-shaking, code splitting, and source maps are built-in without additional plugins, and when I do need custom build flows, the flexible API and plugin architecture have proven more than capable of handling sophisticated optimizations for my node and frontend projects.</p>"
  },
  {
    "name": "zod",
    "ring": "proven",
    "quadrant": "tech-stack",
    "isNew": false,
    "status": "no change",
    "description": "<p><strong><a href=\"https://zod.dev/\">Zod</a></strong> has become my preferred schema validation library for bridging compile-time type safety with runtime validation in TypeScript projects. I value how it allows me to define schemas that validate data structures at runtime while automatically inferring TypeScript types, completely eliminating the need for duplicate type declarations. In my experience, Zod's declarative syntax and support for custom validation have made it exceptionally easy to integrate with frontend frameworks and AI agent workflows. I've found it to be my go-to solution whenever I need robust validation for user input, configuration data, or API responses in TypeScript applications.</p>"
  },
  {
    "name": "luxon",
    "ring": "proven",
    "quadrant": "tech-stack",
    "isNew": false,
    "status": "no change",
    "description": "<p><strong><a href=\"https://moment.github.io/luxon/\">Luxon</a></strong> has become my go-to date/time library for JavaScript, and I've found its modern, immutable approach with chainable APIs to be exactly what I need for handling timezones, internationalization, and precise parsing/formatting. Built on the ECMA Intl API, I've relied on Luxon as my preferred successor to Moment.js across nearly all date management scenarios. In my experience, it excels at converting, manipulating, and displaying time spans, durations, and intervals in both web and Node.js projects. What I value most is its immutability and clear API surface, which have consistently helped me build more maintainable code, while its native browser compatibility makes it my preferred date utility for modern applications and internationalized UIs.</p>"
  },
  {
    "name": "Fastify",
    "ring": "experimental",
    "quadrant": "tech-stack",
    "isNew": false,
    "status": "moved in",
    "description": "<p><strong><a href=\"https://www.fastify.io/\">Fastify</a></strong> has become my preferred web framework for <a href=\"/radar/platforms/node-js\">Node.js</a> development. I value its fast, unopinionated, and low-overhead architecture that delivers all the essential capabilities I need—including parsing, validation, and serialization—without unnecessary bloat. In my experience, the robust plugin system and strong community support have made it easy to extend functionality when needed. I've found no significant downsides compared to alternatives like <a href=\"https://expressjs.com/\">Express.js</a>, while consistently gaining measurable performance improvements across my projects. For minimal yet powerful web development in Node.js, Fastify has proven to be my go-to choice.</p>",
    "messageToConvey": "testing this as a potential replacement for express js"
  },
  {
    "name": "NATS.io",
    "ring": "experimental",
    "quadrant": "tech-stack",
    "isNew": true,
    "status": "new",
    "description": "<p> <strong><a href=\"https://nats.io/\">NATS.io</a></strong> has become my preferred backbone for service-to-service communication in microservices architectures. I've found its combination of low-latency messaging, lightweight deployment, and polyglot client support enables fast, resilient communication without the heavy operational footprint I've experienced with traditional brokers. In my experience, integration into Kubernetes and cloud-native environments has been remarkably straightforward, and I value how NATS' simplicity reduces my need for additional components like service meshes or API gateways. The platform handles publish/subscribe and request/reply patterns out of the box, and I've consistently experienced solid reliability and the scalability needed for real-world production use.</p>",
    "messageToConvey": "great technology. experimenting using this as a service bus, almost like a service mesh of sorts."
  },
  {
    "name": "Restate",
    "ring": "experimental",
    "quadrant": "tech-stack",
    "isNew": false,
    "status": "no change",
    "description": "<p><strong><a href=\"https://restate.dev\">Restate</a></strong> has become my preferred platform for tackling complex distributed system challenges in stateful, fault-tolerant applications. I've found its execution journaling approach particularly valuable—it logs every step, ensuring fault-tolerance, reliable recovery and exactly-once communication across services in a way that's been difficult to achieve with other tools. What I value most architecturally is how Restate separates application logic into three durable service types: Basic Services for stateless functions; Virtual Objects to model concurrent, stateful entities; and Workflows to orchestrate complex, multi-step processes. In my experience assessing Restate within a large insurance system, I've been quite happy with its performance and the clarity it brings to managing distributed state.</p>",
    "messageToConvey": "i dont like it as much as temporal, which is more robust, but experimenting on more lightweight workflows. love the fact that workers can be deployed in lambdas"
  },
  {
    "name": "Model Context Protocol (MCP)",
    "ring": "experimental",
    "quadrant": "ai",
    "isNew": false,
    "status": "moved in",
    "description": "<p>I've found <strong><a href=\"https://modelcontextprotocol.io/\">Model Context Protocol (MCP)</a></strong> to be an increasingly important standard for integrating my LLM applications and agents with external data sources and tools, consistently improving the quality of AI-generated outputs. I value that MCP focuses specifically on context and tool access, which I've learned distinguishes it from the <a href=\"/radar/platforms/agent-to-agent-a2a-protocol\">Agent2Agent (A2A)</a> protocol that governs inter-agent communication. In my experience, its server-client architecture—where servers provide data and tools like databases, wikis and services, while clients (agents, applications and coding assistants) consume them—has proven practical for real-world integrations. Since the last blip, I've observed MCP adoption surge dramatically, with major companies like JetBrains (IntelliJ) and Apple joining the ecosystem, alongside emerging frameworks like <a href=\"/radar/languages-and-frameworks/fastmcp\">FastMCP</a> that I've been exploring. The <a href=\"https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/\">preview MCP Registry</a> standard now supports both public and proprietary tool discovery, which I appreciate for discoverability. However, I've also become aware that MCP's rapid evolution has introduced architectural gaps, with <a href=\"https://julsimon.medium.com/why-mcps-disregard-for-40-years-of-rpc-best-practices-will-burn-enterprises-8ef85ce5bc9b\">valid criticism</a> about overlooking established RPC best practices. For my production applications, I've learned to look <a href=\"https://www.thoughtworks.com/insights/blog/generative-ai/model-context-protocol-beneath-hype\">beyond the hype</a> and apply additional scrutiny by mitigating <a href=\"/radar/techniques/toxic-flow-analysis-for-ai\">toxic flows</a> using tools like <a href=\"/radar/tools/mcp-scan\">MCP-Scan</a> and closely monitoring the <a href=\"https://modelcontextprotocol.io/specification/draft/basic/authorization\">draft Authorization module</a> for security.</p>",
    "messageToConvey": "ai space is changing quickly, mcps are very important now, authorization and guardrails is an important point to pay attentioin to"
  },

  {
    "name": "Retool",
    "ring": "experimental",
    "quadrant": "tech-stack",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://retool.com/\">Retool</a></strong> is a fast-growing platform for building internal tools through low-code assembly. It supports rapid UI development, integration with multiple data sources, and flexible scripting. However, some teams experience vendor lock-in concerns, and scalability for highly customized use cases can be limiting. Retool is worth assessing for scenarios where speed and extensibility outweigh the need for full custom code maturity.</p>",
    "messageToConvey": "good to pump out quick internal tooling"
  },
  {
    "name": "Spec-Driven Development",
    "ring": "experimental",
    "quadrant": "ai",
    "isNew": true,
    "status": "new",
    "description": "<p><strong>Spec-driven development</strong> is an emerging approach to AI-assisted coding workflows that I've been experimenting with extensively. In my experience, this workflow begins with a structured functional specification that I then break down through multiple steps into smaller pieces, solutions and tasks. I've found that my specifications can take many forms: a single document, a set of documents or structured artifacts that capture different functional aspects.</p> <p>I've observed many fellow developers adopt this style (and have developed one of my own that I'm sharing internally at Thoughtworks). I've been particularly interested in three tools that have explored distinct interpretations of spec-driven development. Amazon's <a href=\"https://kiro.dev/\">Kiro</a> has guided me through three workflow stages — requirements, design and tasks creation. <a href=\"https://github.com/github/spec-kit\">GitHub's spec-kit</a> follows a similar three-step process but I value its richer orchestration, configurable prompts and \"constitution\" defining immutable principles that must always be followed. <a href=\"https://tessl.io/\">Tessl Framework</a> (still in private beta as of September 2025) takes what I consider a more radical approach in which the specification itself becomes the maintained artifact, rather than the code. </p> <p>I find this space fascinating, though in my practice the workflows remain elaborate and opinionated. I've noticed these tools behave very differently depending on task size and type; some generate lengthy spec files that I struggle to review effectively, and when they produce PRDs or user stories, I'm sometimes unclear who their intended user is. I suspect we may be relearning a <a href=\"https://en.wikipedia.org/wiki/Bitter_lesson\">bitter lesson</a> — that handcrafting detailed rules for AI ultimately doesn't scale.</p>",
    "messageToConvey": "for complex features, it makes a big different to run a proper workflow to design specs, including multiple agents wearing different hats, almost like simulating a full engineering team. I did find Kiro's approach to spec one of the best so far, although the coding part was not amazing, so using kiro's specs in claude code worked best for me at the time of writing this"
  },
  {
    "name": "Residuality Theory",
    "ring": "experimental",
    "quadrant": "techniques",
    "isNew": true,
    "status": "new",
    "description": "<p><strong>Residuality Theory</strong> is a modern approach to software architecture built on the principles of complexity science and resilience ​. Systems are modeled as \"residues\"—the elements that persist and function after being subjected to unpredictable stressors, such as technical failures or shifting business requirements ​. Unlike traditional component-based methods, Residuality Theory actively incorporates random simulation of stress events to reveal architectural \"attractors\"—preferred states or structures that survive adversity ​​. By repeatedly challenging naive architectures and refining them based on what survives, organizations build antifragile, adaptable systems for chaotic and evolving environments ​. Though the theoretical foundation can be dense, results include more explicit, teachable, and evaluable decision-making for architects, and a vocabulary for resilience that is traceable and practical ​. Residuality Theory is gaining traction among forward-thinking architects seeking tools to both describe and manage software complexity in step with real-world change ​.</p>",
    "messageToConvey": "designing a system that doens't require massive changes or refactorings when requirements change requires thinking and collaboration. I am finding that this technique helps facilitate those conversations and make things that were usually implicit in a senior architect's min, more explicit for the team and the collaboration."
  },
  {
    "name": "Event Modeling",
    "ring": "experimental",
    "quadrant": "techniques",
    "isNew": true,
    "status": "new",
    "description": "<p><strong>Event Modeling</strong> is a visual, scenario-driven method for designing event-based systems by mapping out key events, commands, and state changes across a system's timeline ​. The approach emphasizes outcomes and business value, breaking down processes into incremental \"slices\" that deliver value iteratively while using a clearly defined, ubiquitous language to align business and technical teams ​. Event Modeling incorporates core patterns like commands, events, and queries—often leveraging CQRS and bounded contexts—to support scalability, modularity, and clarity in complex architectures ​. Its adoption increases transparency, fosters cross-disciplinary collaboration, and enables domain-driven design, especially in teams building distributed systems or modern event-driven applications ​.</p>"
  },
  {
    "name": "C4 Model",
    "ring": "experimental",
    "quadrant": "techniques",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://c4model.com/\">C4 Model</a></strong> is a lightweight, hierarchical framework for visualizing and describing software architecture at four levels: context, containers, components, and code ​. Widely adopted by modern engineering and product teams, the C4 Model clarifies structure, boundaries, dependencies, and interactions for systems of any scale. Its notation-agnostic approach enables effective communication between developers, architects, and non-technical stakeholders, supporting better onboarding, risk identification, and collaborative design ​. For effective C4 Model diagramming and collaborative modelling, <strong><a href=\"https://icepanel.io/\">IcePanel</a></strong> is the tool of choice, providing real-time, multi-level support for context, containers, and components, reusable objects, and seamless integration with documentation workflows ​. IcePanel's C4 support helps teams maintain up-to-date architecture, enables agile design, and bridges product/engineering conversations in a single source of truth ​.</p>"
  },
  {
    "name": "AI for code migrations",
    "ring": "experimental",
    "quadrant": "ai",
    "isNew": true,
    "status": "new",
    "description": "<p>Code migrations take many forms — from language rewrites to dependency or framework upgrades — and are rarely trivial, often requiring months of manual effort. One of our teams, when <a href=\"https://www.thoughtworks.com/insights/blog/generative-ai/automating-framework-upgrades-can-combination-of-AI-and-traditional-tooling-help\">upgrading their .NET framework version</a>, experimented with using AI to shorten the process. In the past, we blipped <a href=\"/radar/tools/openrewrite\">OpenRewrite</a>, a deterministic, rule-based refactoring tool. Using AI alone for such upgrades has often proven costly and prone to meandering conversations. Instead, the team combined traditional upgrade pipelines with agentic coding assistants to manage complex transitions. Rather than delegating a full upgrade, they broke the process into smaller, verifiable steps: analyzing compilation errors, generating migration diffs and validating tests iteratively. This hybrid approach positions AI coding agents as pragmatic collaborators in software maintenance. Industry examples, such as <a href=\"https://www.theregister.com/2025/01/16/google_ai_code_migration/\">Google's large-scale int32-to-int64 migration</a>, reflect a similar trend. While our results are mixed in measurable time savings, the potential to reduce developer toil is clear and worth continued exploration.</p>"
  },
  {
    "name": "Structured output from LLMs",
    "ring": "experimental",
    "quadrant": "ai",
    "isNew": false,
    "status": "moved in",
    "description": "<p><strong>Structured output from LLMs</strong> is the practice of constraining a large language model to produce responses in a predefined format — such as JSON or a specific programming class. This technique is essential for building reliable, production-grade applications, transforming the LLM's typically unpredictable text into a machine-readable, deterministic data contract. Based on successful production use, we're moving this technique from Assess to Trial.</p> <p>Approaches range from simple prompt-based formatting and <a href=\"https://platform.openai.com/docs/guides/structured-outputs\">model-native structured outputs</a> to more robust constrained decoding methods using tools like <a href=\"https://github.com/dottxt-ai/outlines\">Outlines</a> and <a href=\"https://github.com/567-labs/instructor\">Instructor</a>, which apply finite-state machines to guarantee valid output. We've successfully used this technique to extract complex, unstructured data from diverse document types and convert it into structured JSON for downstream business logic.</p>"
  },
  {
    "name": "zoxide",
    "ring": "experimental",
    "quadrant": "tools",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://github.com/ajeetdsouza/zoxide\">zoxide</a></strong> is a smart, fast command-line tool that optimizes navigation between directories. By learning common navigation patterns, zoxide enables quick jumps using a minimal CLI. Its adoption leads to marked productivity improvements for engineers who work heavily with terminal workflows.</p>"
  },
  {
    "name": "Claude Code",
    "ring": "proven",
    "quadrant": "ai",
    "isNew": true,
    "status": "new",
    "description": "<p>Anthropic's <strong><a href=\"https://claude.com/product/claude-code\">Claude Code</a></strong> is an agentic AI coding tool that provides a natural language interface and agentic execution model for planning and implementing complex, multi-step workflows. Released less than a year ago, it has already been widely adopted by developers inside and outside Thoughtworks, leading us to place it in Trial. Console-based coding agents such as OpenAI's <a href=\"https://developers.openai.com/codex/cli/\">Codex CLI</a>, Google's <a href=\"https://developers.google.com/gemini-code-assist/docs/gemini-cli\">Gemini CLI</a> and the open-source <a href=\"https://apidog.com/blog/opencode/\">OpenCode</a> have been released, while IDE-based assistants like <a href=\"/radar/tools/cursor\">Cursor</a>, <a href=\"/radar/tools/windsurf\">Windsurf</a> and <a href=\"/radar/tools/github-copilot\">GitHub Copilot</a> now include agent modes. Even so, Claude Code remains a favorite. We see teams using it not only to write and modify code but also as a general-purpose AI agent for managing specifications, stories, configuration, infrastructure and documentation.</p> <p>Agentic coding shifts the developer's focus from writing code to specifying intent and delegating implementation. While this can accelerate development cycles, it can also lead to <a href=\"/radar/techniques/complacency-with-ai-generated-code\">complacency with AI-generated code</a>, which in turn may result in code that is harder to maintain and evolve — for both humans and AI agents. It's therefore essential for teams to rigorously manage how Claude Code works, using techniques such as <a href=\"/radar/techniques/context-engineering\">context engineering</a>, <a href=\"/radar/techniques/curated-prompt-libraries-for-software-teams\">curated shared instructions</a> and potentially <a href=\"/radar/techniques/team-of-coding-agents\">teams of coding agents</a>.</p>"
  },
  {
    "name": "Context7",
    "ring": "experimental",
    "quadrant": "ai",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://github.com/upstash/context7\">Context7</a></strong> is an <a href=\"/radar/platforms/model-context-protocol-mcp\">MCP</a> server that addresses inaccuracies in AI generated code. While LLMs rely on outdated training data, Context7 ensures they generate accurate, up-to-date and version-specific code for the libraries and frameworks used in a project. It does this by pulling the latest documentation and functional code examples directly from framework source repositories and injecting them into the LLM's context window at the moment of prompting. In our experience, Context7 has greatly reduced code hallucinations and reliance on stale training data. You can configure it with AI code editors such as Claude Code, Cursor or VS Code to generate, refactor or debug framework-dependent code.</p>"
  },
  {
    "name": "Go Rules",
    "ring": "experimental",
    "quadrant": "tools",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://github.com/gorules/actor\">GoRules</a></strong> is a rules engine for complex decision logic written in Go. While GoRules brings a flexible, embeddable architecture and strong type safety, the ecosystem is still maturing. Teams leveraging Go and requiring business rules should assess GoRules against alternatives like Drools or Open Policy Agent for fit and maintainability.</p>"
  },
  {
    "name": "Agent Development Kit (ADK)",
    "ring": "learning",
    "quadrant": "ai",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://google.github.io/adk-docs/\">Agent Development Kit</a> (ADK)</strong> is a framework for developing and deploying AI agents that applies modern software engineering discipline rather than relying solely on prompting. It introduces familiar abstractions such as classes, methods, workflow patterns and CLI support. Compared to frameworks like <a href=\"/radar/languages-and-frameworks/langgraph\">LangGraph</a> or <a href=\"/radar/languages-and-frameworks/crewai\">CrewAI</a>, ADK's strength lies in its deep integration with Google's AI infrastructure, providing enterprise-ready grounding, data access and monitoring. It's also designed for interoperability, supporting tool wrappers and the <a href=\"/radar/platforms/agent-to-agent-a2a-protocol\">A2A protocol</a> for agent-to-agent communication. For organizations already invested in GCP, ADK presents a promising foundation for building scalable, secure and manageable agentic architecture. Though still early in its evolution, it signals Google's direction toward a native, full-stack agent development environment. We recommend keeping a close eye on its maturity and ecosystem growth.</p>"
  },
  {
    "name": "htmx",
    "ring": "learning",
    "quadrant": "tech-stack",
    "isNew": false,
    "status": "no change",
    "description": "<p><strong><a href=\"https://htmx.org/\">htmx</a></strong> is a small, dependency-free UI library that has gained significant popularity since 2023 for its HTML-first approach to interactivity, evoking the simplicity of early web development ​. Unlike modern JavaScript frameworks, htmx enables AJAX, CSS transitions, WebSockets, and server-sent events through declarative HTML attributes rather than complex build pipelines or client-side code ​. This philosophy—sometimes called hypertext-driven development—appeals to developers seeking to minimize JavaScript and return to server-centric architectures ​. htmx's rapid adoption, nostalgic ethos, and lightweight implementation (roughly 14kB gzipped) make it ideal for small to medium apps and incremental adoption in existing stacks, though scalability and advanced client-side UIs can be limiting ​. The project's team emphasizes stability and thoughtful expansion, often through extensions rather than major core changes ​. Essays on the htmx website offer further insight into its philosophy and vision for modern web development ​.</p>"
  },
  {
    "name": "Qwik",
    "ring": "learning",
    "quadrant": "tech-stack",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://qwik.dev/\">Qwik</a></strong> is a next-generation frontend framework rethinking web performance and interactivity through its novel \"resumability\" model ​. Unlike React or Vue, Qwik serializes application state on the server and pauses execution, which resumes seamlessly on the client. This approach eliminates traditional hydration and significantly reduces initial JavaScript payloads—most Qwik apps start with only HTML, loading code on demand for interactive components ​. Qwik's architecture enables sub-second full page loads, fine-grained lazy loading, and strong SEO, making it ideal for performance-critical sites, dynamic dashboards, and scale-sensitive projects ​. Its ecosystem is growing, but adoption should be assessed relative to production stability and plugin maturity for complex, highly interactive applications ​.</p>"
  },
  {
    "name": "n8n",
    "ring": "learning",
    "quadrant": "tools",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://github.com/n8n-io/n8n\">n8n</a></strong> is a fair-code–licensed workflow automation platform, similar to <a href=\"https://zapier.com/\">Zapier</a> or <a href=\"https://www.make.com/\">Make</a> (formerly Integromat), but built for developers who want a self-hosted, extensible and code-controllable option. It offers a lower-code, visual approach to workflow creation than <a href=\"https://airflow.apache.org/\">Apache Airflow</a>, while still supporting custom code in JavaScript or Python.</p> <p>Its primary use case is integrating multiple services into automated workflows, but it can also connect LLMs with configurable data sources, memory and tools. Many of our teams use n8n to rapidly prototype agentic workflows triggered by chat applications or webhooks, often leveraging its import and export capabilities to generate workflows with AI assistance. As always, we advise caution when using <a href=\"/radar/platforms/low-code-platforms\">low-code platforms</a> in production. However, n8n's self-hosting and code-defined workflows can mitigate some of those risks.</p>"
  },
  {
    "name": "Google Cloud Vertex AI",
    "ring": "learning",
    "quadrant": "tech-stack",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://cloud.google.com/vertex-ai\">Google Cloud Vertex AI</a></strong> has become increasingly central to my AI engineering practice since its major advancements in May 2023. I've found the <strong>Model Garden</strong> particularly valuable—having access to 100+ pre-trained and open models (including third-party and Google's own foundation models) in one place has accelerated my prototyping significantly. The <strong>Generative AI Studio</strong> is now my go-to console for rapid experimentation with generative models before committing to production implementations. I've been impressed by how <strong>Vertex AI Extensions</strong> enable managed connections between models and real-time data, APIs, or enterprise actions—this has unlocked integration patterns I previously had to build from scratch. In my experience working across text-to-image, code generation, and speech-to-text projects, models like PaLM 2, Imagen, Codey, and Chirp have proven robust and reliable for production use. What I value most are the enterprise-grade features I need for real-world deployments: reinforcement learning from human feedback for fine-tuning, comprehensive security controls, grounding capabilities in enterprise data, and digital watermarking for content provenance. These rapid developments have made Vertex AI my preferred managed AI suite when working within the Google Cloud ecosystem, and I've found its evolving generative and integration features consistently meet my expanding requirements.</p>"
  },
  {
    "name": "ClickHouse",
    "ring": "learning",
    "quadrant": "tech-stack",
    "isNew": false,
    "status": "moved in",
    "description": "<p><strong><a href=\"https://clickhouse.com/\">ClickHouse</a></strong> has become my preferred open-source columnar OLAP database for real-time analytics at scale. I've found its performance and scalability impressive across large-scale data analytics projects I've worked on. What I value most are its incremental materialized views, efficient query engine and strong data compression—features that have consistently made interactive queries fast and cost-effective in my experience. I appreciate how the built-in support for approximate aggregate functions enables me to make smart trade-offs between accuracy and performance, which has proven especially useful for high-cardinality analytics scenarios. The addition of the S3 storage engine with MergeTree has been a game-changer in my practice, allowing me to cleanly separate storage and compute using S3-compatible storage for ClickHouse tables. In my experience, ClickHouse has also been an excellent backend for <a href=\"https://clickhouse.com/blog/clickhouse-and-open-telemtry\">OpenTelemetry</a> data and crash analytics tools like <a href=\"/radar/tools/sentry\">Sentry</a>. For my analytics workloads requiring a fast, open-source engine, ClickHouse has become my go-to choice.</p>"
  },
  {
    "name": "Agent-to-Agent (A2A) Protocol",
    "ring": "learning",
    "quadrant": "ai",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://github.com/a2aproject/A2A\">Agent2Agent (A2A)</a></strong> is a protocol that defines a standard for communication and interaction among agents in complex, multi-agent workflows. It uses <em>Agent Cards</em> to describe the key elements of inter-agent communication, including skill discovery and the specification of transport and security schemes. A2A complements the <a href=\"/radar/platforms/model-context-protocol-mcp\">Model Context Protocol</a> (MCP) by focusing on agent-to-agent communication without exposing internal details such as an agent's state, memory or internal. </p> <p>The protocol promotes best practices such as an asynchronous-first approach for long-running tasks, streaming responses for incremental updates and secure transport with HTTPS, authentication and authorization. SDKs are available in Python, JavaScript, Java and C# to facilitate rapid adoption. Although relatively new, A2A enables teams to build domain-specific agents that can collaborate to form complex workflows, making it a strong option for such scenarios.</p>"
  },
  {
    "name": "OpenFeature",
    "ring": "learning",
    "quadrant": "tech-stack",
    "isNew": true,
    "status": "new",
    "description": "<p>As businesses scale, feature flag management often becomes increasingly complex; teams need an abstraction layer that goes beyond the <a href=\"/radar/techniques/simplest-possible-feature-toggle\">simplest possible feature toggle</a>. <strong><a href=\"https://openfeature.dev/\">OpenFeature</a></strong> provides this layer through a vendor-agnostic, community-driven API specification that standardizes how feature flags are defined and consumed, decoupling application code from the management solution. This flexibility allows teams to switch providers easily — from basic setups using environment variables or in-memory configurations up to mature platforms like <a href=\"/radar/tools/configcat\">ConfigCat</a> or <a href=\"https://launchdarkly.com/\">LaunchDarkly</a>. However, one critical caution remains: teams must manage different <a href=\"https://martinfowler.com/articles/feature-toggles.html#CategoriesOfToggles\">categories of flags</a> separately and with discipline to avoid flag proliferation, application complexity and excessive testing overhead.</p>"
  },
  {
    "name": "Databricks",
    "ring": "learning",
    "quadrant": "tech-stack",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://databricks.com/\">Databricks</a></strong> provides a unified analytics platform based on Apache Spark, offering powerful capabilities for big data, AI, and machine learning workloads. Databricks excels in collaboration, performance, and integration with cloud ecosystems. Organizations should assess TCO and operational complexity, especially compared to simpler serverless options, but its feature set remains a reference point for modern data platforms.</p>"
  },
  {
    "name": "Context engineering",
    "ring": "learning",
    "quadrant": "ai",
    "isNew": true,
    "status": "new",
    "description": "<p><strong>Context engineering</strong> is the systematic design and optimization of the information provided to a large language model during inference to reliably produce the desired output. It involves structuring, selecting and sequencing contextual elements — such as prompts, retrieved data, memory, instructions and environmental signals — so the model's internal layers operate in an optimal state. Unlike prompt engineering, which focuses only on the wording of prompts, context engineering considers the entire configuration of context: how relevant knowledge, instructions and prior context are organized and delivered to achieve the most effective results.</p> <p>Today, engineers use a range of discrete techniques that can be grouped into three areas: <em>Context setup</em> covers curation tactics such as using minimal <a href=\"https://www.promptingguide.ai/introduction/tips\">system prompts</a>, canonical <a href=\"https://www.promptingguide.ai/techniques/fewshot\">few-shot examples</a> and token-efficient <a href=\"https://www.anthropic.com/engineering/writing-tools-for-agents\">tools</a> for decisive action. <em>Context management for long-horizon tasks</em> addresses finite context windows through <a href=\"https://blog.langchain.com/context-engineering-for-agents/\">context summarization</a> , <a href=\"https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents\">structured note-taking</a> to persist external memories and <a href=\"https://docs.claude.com/en/docs/claude-code/sub-agents\">sub-agent architectures</a> to isolate and summarize complex sub-tasks. <em>Dynamic information retrieval</em> relies on <a href=\"https://jentic.com/blog/just-in-time-tooling\">just-in-time (JIT) context retrieval</a>, where agents autonomously load external data only when immediately relevant, maximizing efficiency and precision.</p>"
  },
  {
    "name": "Team of coding agents",
    "ring": "learning",
    "quadrant": "ai",
    "isNew": true,
    "status": "new",
    "description": "<p>In my development practice, I've found that orchestrating a <strong>team of coding agents</strong>—each with a distinct role such as architect, back-end specialist, or tester—has become an increasingly valuable technique for tackling complex development tasks. I've been experimenting with tools like <a href=\"/radar/tools/claude-code\">Claude Code</a>, <a href=\"https://github.com/RooCodeInc/Roo-Code\">Roo Code</a> and <a href=\"https://kilocode.ai/\">Kilo Code</a> which support subagents and multiple operating modes. My experience aligns with the proven principle that assigning LLMs specific roles and personas improves output quality—I've found that coordinating multiple role-specific agents consistently delivers better results than relying on a single general-purpose one. In my exploration, I'm still discovering the optimal level of agent abstraction, and I've observed it can extend beyond simple one-to-one mappings with traditional team roles. This approach represents a shift in how I think about AI-assisted development: moving from single-agent interactions toward orchestrated, multi-step pipelines that mirror collaborative team dynamics.</p>"
  },
  {
    "name": "GenAI for forward engineering",
    "ring": "learning",
    "quadrant": "ai",
    "isNew": true,
    "status": "new",
    "description": "<p><strong>GenAI for forward engineering</strong> has become my preferred technique for modernizing legacy systems. I've found that having GenAI generate descriptions of legacy codebases creates an invaluable explicit step focused on <em>what</em> the legacy code does (its specification) while deliberately hiding <em>how</em> it's currently implemented. In my experience, this relates closely to <a href=\"radar/techniques/spec-driven-development\">spec-driven development</a> but proves especially powerful when applied to legacy modernization. </p> <p>What I value most is how generating and iterating on functional descriptions before rewriting code helps me surface hidden logic, dependencies and edge cases that I might otherwise overlook. I've discovered that emphasizing the problem space rather than the existing system allows me to leverage GenAI models to explore more creative and forward-looking solutions. My preferred workflow follows a reverse-engineering → design/solutioning → forward-engineering loop, which enables both myself and AI agents to reason at a higher level before committing to an implementation.</p> <p>In my work at Thoughtworks, I've successfully applied this approach across multiple teams to accelerate legacy system rewrites. What I appreciate is that the goal isn't to obscure implementation details entirely, but to introduce a temporary abstraction that helps me and my teams explore alternatives without being constrained by the current structure. I've consistently found this technique produces cleaner, more maintainable and future-ready code while significantly reducing the time I spend understanding existing implementations.</p>"
  },
  {
    "name": "Serena",
    "ring": "learning",
    "quadrant": "tools",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://github.com/oraios/serena\">Serena</a></strong> has become an essential toolkit in my practice for equipping coding agents like Claude Code with IDE-like capabilities for semantic code retrieval and editing. I've found that its symbol-level operations and understanding of code's relational structure dramatically improve token efficiency in ways I couldn't achieve otherwise. In my experience, being able to use precise Serena <a href=\"https://github.com/oraios/serena?tab=readme-ov-file#list-of-tools\">tools</a> such as find_symbol, find_referencing_symbols and insert_after_symbol—instead of reading entire files or relying on crude string replacements—has transformed how coding agents locate and edit code. While I've observed the impact is minimal on small projects, this efficiency has proven extremely valuable as my codebases grow in complexity and scale.</p>"
  },
  {
    "name": "Convex",
    "ring": "learning",
    "quadrant": "tools",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://convex.dev/\">Convex</a></strong> has caught my attention as a backend-as-a-service for real-time web applications, offering an integrated database, reactive queries, and serverless functions in a unified platform. I've found the developer experience, type safety, and live data sync capabilities genuinely appealing in my early explorations. However, I'm mindful that the ecosystem is still evolving and hasn't yet reached the maturity I've come to expect from more established platforms. In my experience evaluating it for production workloads, I believe Convex shows particular promise for collaborative applications and scenarios with demanding real-time UX requirements, though I recommend careful assessment before committing to production use.</p>"
  },
  {
    "name": "KEDA",
    "ring": "learning",
    "quadrant": "tools",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://keda.sh/\">KEDA</a></strong> (Kubernetes-based Event Driven Autoscaling) has become my preferred solution for event-driven autoscaling in Kubernetes environments. I've found its ability to scale workloads based on messaging queues and external API triggers fills a critical gap that native Kubernetes autoscaling doesn't address. In my experience, KEDA delivers cost-effective scaling for event-driven architectures, though I recommend thorough trials to validate production readiness and ensure compatibility with your existing autoscaling strategies.</p>"
  },
  {
    "name": "Surreal DB",
    "ring": "learning",
    "quadrant": "tools",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://surrealdb.com/\">SurrealDB</a></strong> has caught my attention as a multi-model database that I've been exploring for projects requiring both NoSQL flexibility and relational structure. I value its native support for graph queries, web APIs, and dynamic schemas—features I've found particularly useful when prototyping early-stage applications. In my experience, SurrealDB has become my go-to choice for projects that need to blend traditional relational data with rich document and graph relationships, such as AI agent memory systems, recommendation engines, knowledge graphs, fraud detection platforms, and live collaborative applications. What I appreciate most is its unified multi-model design, which has allowed me to handle diverse query patterns and real-time analytics without the overhead of managing multiple database platforms. That said, as a newer product in the ecosystem, I've learned to carefully validate its stability and production readiness through thorough trials before committing to critical workloads.</p>"
  },
  {
    "name": "Refine.dev",
    "ring": "learning",
    "quadrant": "tools",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://refine.dev/\">Refine.dev</a></strong> has become my preferred React-based framework for building data-centric internal tools. In my experience, Refine accelerates development significantly with its customizable components, broad integrations, and strong type system. I value it as an alternative to proprietary low-code platforms, though I've found it essential to trial its flexibility and integration depth when tackling more complex use cases before committing to production deployments.</p>"
  },
  {
    "name": "astro.build",
    "ring": "learning",
    "quadrant": "tools",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://astro.build/\">Astro</a></strong> has become my preferred framework for building content-focused websites where performance is paramount. I've found its zero-JS by default approach and component island architecture deliver the ultra-fast web experiences I value most, without sacrificing developer ergonomics. In my experience, Astro's framework-agnostic design has made it remarkably easy to integrate my existing React, Vue, or Svelte components while shipping minimal JavaScript to the browser. I appreciate how its predictable performance model eliminates the guesswork around bundle sizes and hydration costs. While I've seen impressive gains in my early projects, I'm continuing to assess how well it scales across larger organizational patterns before recommending it for full-scale adoption.</p>"
  },
  {
    "name": "Linear",
    "ring": "learning",
    "quadrant": "tools",
    "isNew": true,
    "status": "new",
    "description": "<p><strong><a href=\"https://linear.app/\">Linear</a></strong> has become my preferred project management and issue tracking tool for software development work. I value its developer-first principles—keyboard-centric navigation, blazing-fast updates, and opinionated workflows—which I've found enable streamlined sprint planning, backlog triage, and milestone tracking without the friction I've experienced in other tools. In my experience, the built-in cycles (sprints), customizable issue templates, roadmapping capabilities, and powerful integrations with GitHub, Slack, and Figma have proven essential for my daily workflow. I've been particularly impressed by Linear's AI-powered features, such as issue summarization, auto-prioritization, and sprint analytics, which have noticeably reduced the manual work I used to spend on project organization. What I appreciate most is Linear's speed, focus, and cross-team visibility in technical work environments. That said, I've observed its limited customization may challenge adoption in non-technical contexts or teams requiring flexible business processes. For agile product teams seeking frictionless planning, real-time collaboration, and integration-rich development workflows, I've found Linear to be an excellent choice.</p>"
  },
  {
    "name": "Capacity Driven Development",
    "ring": "avoid",
    "quadrant": "techniques",
    "isNew": true,
    "status": "new",
    "description": "<p>In my experience architecting stream-aligned teams, I've found that maintaining focus on a single, valuable flow—such as a user journey or product—is key to delivering end-to-end value efficiently. However, I've observed a concerning trend I call <strong>capacity-driven development</strong>, where well-aligned teams take on features from other products or streams when they have spare capacity. While this may appear efficient in the short term, I've learned it's a local optimization best suited only for handling sudden spikes in demand. When I've seen this pattern normalized, it consistently increases cognitive load and technical debt, and in the worst cases I've witnessed it lead to congestion collapse as the cost of context switching across products compounds. In my practice, teams with <a href=\"https://martinfowler.com/bliki/Slack.html\">spare capacity</a> are far better served by focusing on improving <a href=\"/radar/techniques/tracking-health-over-debt\">system health</a>. My preferred approaches to managing capacity effectively include using WIP limits to control work across adjacent streams, cross-training to balance periods of high demand, and applying <a href=\"https://agilealliance.org/resources/experience-reports/dynamic-reteaming-how-we-thrive-by-rebuilding-teams/\">dynamic reteaming techniques</a> when needed.</p>"
  },
  {
    "name": "Naive API-to-MCP conversion",
    "ring": "avoid",
    "quadrant": "ai",
    "isNew": true,
    "status": "new",
    "description": "<p>In my experience working with AI agents, I've observed a strong desire among teams to enable agent interactions with existing systems through what appears to be a seamless, direct conversion of internal APIs to the <a href=\"/radar/platforms/model-context-protocol-mcp\">Model Context Protocol (MCP)</a>. I've seen a growing number of tools, such as <a href=\"https://github.com/automation-ai-labs/mcp-link\">MCP link</a> and <a href=\"https://github.com/tadata-org/fastapi_mcp\">FastAPI-MCP</a>, emerge to support this conversion.</p> <p>However, I've learned to strongly avoid this <strong>naive API-to-MCP conversion</strong> approach. In my practice, I've found that APIs designed for human developers typically consist of granular, atomic actions that, when chained together by an AI, lead to excessive token usage, context pollution, and poor agent performance. What concerns me most is that these APIs—especially internal ones—frequently expose sensitive data or allow destructive operations. While I've successfully mitigated such risks through architecture patterns and code reviews when working with human developers, I've found there's no reliable, deterministic way to prevent an autonomous AI agent from misusing such endpoints when they're naively exposed via MCP. My preferred approach is to architect a dedicated, secure MCP server specifically tailored for agentic workflows, built on top of existing APIs.</p>"
  },
  {
    "name": "Standalone data engineering teams",
    "ring": "avoid",
    "quadrant": "techniques",
    "isNew": true,
    "status": "new",
    "description": "<p>In my experience, organizing <strong>separate data engineering teams</strong> to develop and own data pipelines and products — isolated from the <a href=\"https://teamtopologies.com/key-concepts\">stream-aligned</a> business domains they serve — has consistently proven to be an anti-pattern that I've seen lead to inefficiencies and weak business outcomes. I've watched this structure repeat past mistakes I encountered with isolating <a href=\"/radar/techniques/separate-devops-team\">DevOps</a>, <a href=\"/radar/techniques/testing-as-a-separate-organization\">testing</a> or <a href=\"/radar/techniques/separate-code-and-pipeline-ownership\">deployment</a> functions, inevitably creating knowledge silos, bottlenecks and wasted effort. What I've found is that without close collaboration, data engineers often lack the business and domain context I consider essential for designing meaningful data products, which in my observation limits both adoption and value. My preferred approach is for data platform teams to focus on maintaining the shared infrastructure, while cross-functional business teams build and own their <a href=\"/radar/techniques/data-product-thinking\">data products</a>, following <a href=\"/radar/techniques/data-mesh\">data mesh</a> principles. I strongly advocate against siloed organizational patterns like this — especially as the need for domain-rich, AI-ready data continues to grow in the systems I architect.</p>"
  }
]
